# -*- coding: utf-8 -*-
"""CKD_SMOTE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHVHKu43qaz-ZEgQrxIAx73DjdL9JQ2Y

# Install Libraries
"""

!pip install tensorflow pandas numpy sklearn

!pip install pandas scikit-learn
!pip install openpyxl

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.metrics import AUC
from tensorflow.keras.metrics import Precision, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, AUC
from sklearn.metrics import accuracy_score, recall_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import random
import tensorflow as tf
from sklearn.impute import KNNImputer
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
plt.style.use('fivethirtyeight')
sns.set()
plt.style.use('ggplot')
# %matplotlib inline
from google.colab import drive

drive.mount('/content/drive')

data_path_UCI = '/content/drive/MyDrive/CKD/Data/UCIdataset/UCI_kidney_disease.csv'

"""# UCI dataset

## 1. Data stats
"""

df = pd.read_csv(data_path_UCI)

df.dtypes

df.isna().sum()

df.info()

df.drop('id', axis=1, inplace = True)
df.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',
              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',
              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',
              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',
              'aanemia', 'class']

df.shape

df['packed_cell_volume'] = pd.to_numeric(df['packed_cell_volume'], errors='coerce')
df['white_blood_cell_count'] = pd.to_numeric(df['white_blood_cell_count'], errors='coerce')
df['red_blood_cell_count'] = pd.to_numeric(df['red_blood_cell_count'], errors='coerce')

df.columns

df['serum_creatinine'].head()

df['serum_creatinine'].info()

cat_cols = [col for col in df.columns if df[col].dtype == 'object']
num_cols = [col for col in df.columns if df[col].dtype != 'object']

for col in cat_cols:
    print(f"{col}: {df[col].unique()}")

df['diabetes_mellitus'].replace(to_replace = { '\tno':'no', '\tyes':'yes', ' yes':'yes'}, inplace=True)
df['coronary_artery_disease'].replace(to_replace = {'\tno':'no'}, inplace = True)
df['class'].replace(to_replace = {'ckd\t':'ckd', 'notckd': 'not ckd'}, inplace = True)

for col in cat_cols:
    print(f"{col}: {df[col].unique()}")

df['class'] = df['class'].map({'ckd':0, 'not ckd':1})
df['class'] = pd.to_numeric(df['class'], errors = 'coerce')

for col in ['diabetes_mellitus','coronary_artery_disease','class' ]:
  print(f" {col} has {df[col].unique()} ")

"""##Data pre-processing"""

df.head(10)

num_data_missing = df[num_cols]
cat_data_missing = df[cat_cols]

for col in cat_cols:
  print(f"{col}: {df[col].unique()}")

def knn_impute(df, num_cols, n_neighbors=5):
    num_data_missing = df[num_cols]
    knn_imputer = KNNImputer(n_neighbors=n_neighbors)
    num_data_imputed = pd.DataFrame(knn_imputer.fit_transform(num_data_missing), columns=num_cols)

    # Update the original DataFrame with imputed values
    df[num_cols] = num_data_imputed
    return df

df_imputed = knn_impute(df, num_cols)

for col in cat_cols:
    df_imputed[col].fillna(df_imputed[col].mode()[0], inplace=True)

df_imputed.isna().sum()

for col in cat_cols:
  print(f"{col}: {df_imputed[col].unique()}")

df.head()

"""##Feature encoding"""

for col in cat_cols:
    print(f"{col} has {df[col].unique()}")

# label_encoder
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
# print("Label Mapping:", label_mapping)

df.head()

df_imputed.info()

"""##Model Building"""

from imblearn.over_sampling import SMOTE
X = df_imputed.drop(columns=['class'])
y = df_imputed['class']

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler

# Models and their hyperparameter grids
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'XGBoost': xgb.XGBClassifier(random_state=42),
    'LightGBM': lgb.LGBMClassifier(random_state=42),
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
}

# Hyperparameter grids for GridSearchCV
param_grids = {
    'RandomForest': {'max_depth': [10, 15, 20], 'n_estimators': [50, 100, 200], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]},
    'XGBoost': {'max_depth': [3, 6, 9], 'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1], 'subsample': [0.8, 1.0], 'colsample_bytree': [0.8, 1.0]},
    'LightGBM': {'max_depth': [10, 15, 20], 'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1], 'feature_fraction': [0.8, 1.0]},
    'DecisionTree': {'max_depth': [10, 15, 20], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]},
    'LogisticRegression': {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2'], 'solver': ['lbfgs']}
}

# Set random seeds
seed_value = 0
np.random.seed(seed_value)

# Parameters
n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
df_results_simple = pd.DataFrame(columns=['Model', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives',
                                          'TPR', 'TNR', 'FPR', 'FNR',
                                          'Accuracy', 'Precision', 'Recall', 'f1_score', 'Specificity', 'ROC-AUC'])
feature_importances_dict = {}

def normalize_importances(importances):
    norm_importances = importances / np.sum(importances)
    return norm_importances

def collect_feature_importances(model, model_name, X_columns, feature_importances_dict):
    if hasattr(model, 'feature_importances_'):
        importances = normalize_importances(model.feature_importances_)
    elif model_name == "LogisticRegression":
        importances = normalize_importances(np.abs(model.coef_[0]))
    else:
        return

    for feature, importance in zip(X_columns, importances):
        if feature in feature_importances_dict:
            feature_importances_dict[feature].append(importance)
        else:
            feature_importances_dict[feature] = [importance]

def evaluate_simple_model(model_name, model, X, y,feature_importances_dict):
    all_y_true = []
    all_y_pred = []
    for train_idx, test_idx in skf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply SMOTE to the training set
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        # Scale the data
        scaler = StandardScaler()
        X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)
        X_test_scaled = scaler.transform(X_test)

        # Train the model
        model.fit(X_train_resampled_scaled, y_train_resampled)

        # Evaluate the model
        y_pred = model.predict(X_test_scaled)
        predictions = model.predict_proba(X_test_scaled)[:, 1]

        # Collect true labels and predictions for confusion matrix
        all_y_true.extend(y_test)
        all_y_pred.extend(y_pred)
        collect_feature_importances(model, model_name, X.columns, feature_importances_dict)


        # Compute the statistics
        true_positives = np.sum((y_pred == 1) & (y_test == 1))
        true_negatives = np.sum((y_pred == 0) & (y_test == 0))
        false_positives = np.sum((y_pred == 1) & (y_test == 0))
        false_negatives = np.sum((y_pred == 0) & (y_test == 1))
        tpr = true_positives / (true_positives + false_negatives)
        tnr = true_negatives / (true_negatives + false_positives)
        fpr = false_positives / (false_positives + true_negatives)
        fnr = false_negatives / (false_negatives + true_positives)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = 2 * precision * recall / (precision + recall)
        specificity = true_negatives / (true_negatives + false_positives)
        roc_auc = roc_auc_score(y_test, predictions)

        # Add the statistics to the dataframe
        df_results_simple.loc[len(df_results_simple)] = [model_name, true_positives, true_negatives, false_positives, false_negatives,
                                                        tpr, tnr, fpr, fnr, accuracy, precision, recall, f1, specificity, roc_auc]

    return all_y_true, all_y_pred

# Evaluate each simple model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")
    all_y_true, all_y_pred = evaluate_simple_model(model_name, model, X, y,feature_importances_dict)
    final_confusion_matrix = confusion_matrix(all_y_true, all_y_pred)
    print(f"Final Confusion Matrix for {model_name}:\n", final_confusion_matrix)
    plt.figure(figsize=(10, 7))
    sns.heatmap(final_confusion_matrix, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()

print("Overall Results for Simple Models:\n", df_results_simple.groupby('Model').mean())


average_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importances_dict.items()}

# Sort features by importance
sorted_features = sorted(average_feature_importances.items(), key=lambda item: item[1], reverse=True)

# Print sorted features by importance
print("Overall Feature Importances (from highest to lowest):")
for feature, importance in sorted_features:
    print(f"{feature}: {importance}")

# Plot the feature importances
features = [item[0] for item in sorted_features]
importances = [item[1] for item in sorted_features]

plt.figure(figsize=(12, 8))
plt.barh(features, importances, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()  # highest importance at the top
plt.show()

df_results_tuned = pd.DataFrame(columns=['Model', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives',
                                         'TPR', 'TNR', 'FPR', 'FNR',
                                         'Accuracy', 'Precision', 'Recall', 'f1_score', 'Specificity', 'ROC-AUC'])
feature_importances_dict = {}

def evaluate_tuned_model(model_name, model, param_grid, X, y,feature_importances_dict):
    all_y_true = []
    all_y_pred = []
    for train_idx, test_idx in skf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply SMOTE to the training set
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        # Scale the data
        scaler = StandardScaler()
        X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)
        X_test_scaled = scaler.transform(X_test)

        # Perform GridSearchCV
        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc')
        grid_search.fit(X_train_resampled_scaled, y_train_resampled)
        best_model = grid_search.best_estimator_

        # Make predictions
        y_pred = best_model.predict(X_test_scaled)
        predictions = best_model.predict_proba(X_test_scaled)[:, 1]

        # Collect true labels and predictions for confusion matrix
        all_y_true.extend(y_test)
        all_y_pred.extend(y_pred)
        collect_feature_importances(model, model_name, X.columns, feature_importances_dict)

        # Compute the statistics
        true_positives = np.sum((y_pred == 1) & (y_test == 1))
        true_negatives = np.sum((y_pred == 0) & (y_test == 0))
        false_positives = np.sum((y_pred == 1) & (y_test == 0))
        false_negatives = np.sum((y_pred == 0) & (y_test == 1))
        tpr = true_positives / (true_positives + false_negatives)
        tnr = true_negatives / (true_negatives + false_positives)
        fpr = false_positives / (false_positives + true_negatives)
        fnr = false_negatives / (false_negatives + true_positives)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = 2 * precision * recall / (precision + recall)
        specificity = true_negatives / (true_negatives + false_positives)
        roc_auc = roc_auc_score(y_test, predictions)

        # Add the statistics to the dataframe
        df_results_tuned.loc[len(df_results_tuned)] = [model_name, true_positives, true_negatives, false_positives, false_negatives,
                                                      tpr, tnr, fpr, fnr, accuracy, precision, recall, f1, specificity, roc_auc]

    return all_y_true, all_y_pred

# Evaluate each tuned model
for model_name in models.keys():
    print(f"Evaluating {model_name} with GridSearchCV...")
    model = models[model_name]
    param_grid = param_grids[model_name]
    all_y_true, all_y_pred = evaluate_tuned_model(model_name, model, param_grid, X, y,feature_importances_dict)
    final_confusion_matrix = confusion_matrix(all_y_true, all_y_pred)
    print(f"Final Confusion Matrix for {model_name} (Tuned):\n", final_confusion_matrix)
    plt.figure(figsize=(10, 7))
    sns.heatmap(final_confusion_matrix, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix for {model_name} (Tuned)')
    plt.show()

print("Overall Results for Hyperparameter Tuned Models:\n", df_results_tuned.groupby('Model').mean())

# Combine results for comparison
df_results_combined = pd.concat([df_results_simple.assign(Version='Simple'),
                                 df_results_tuned.assign(Version='Tuned')])

# Print the combined results for comparison
print("Combined Results:\n", df_results_combined.groupby(['Model', 'Version']).mean())
average_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importances_dict.items()}

# Sort features by importance
sorted_features = sorted(average_feature_importances.items(), key=lambda item: item[1], reverse=True)

# Print sorted features by importance
print("Overall Feature Importances (from highest to lowest):")
for feature, importance in sorted_features:
    print(f"{feature}: {importance}")

# Plot the feature importances
features = [item[0] for item in sorted_features]
importances = [item[1] for item in sorted_features]

plt.figure(figsize=(12, 8))
plt.barh(features, importances, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()  # highest importance at the top
plt.show()

# Visualize the differences using plots
metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'f1_score']

def plot_metrics(metric):
    plt.figure(figsize=(14, 10))
    sns.boxplot(x='Model', y=metric, hue='Version', data=df_results_combined)
    plt.title(f'{metric} Comparison between Simple and Tuned Models')
    plt.show()

# Plot each metric
for metric in metrics:
    plot_metrics(metric)